{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca6b724-2fd5-46cf-90fd-d4a5b68b50ed",
   "metadata": {},
   "source": [
    "# MADS-Deep Learning\n",
    "---\n",
    "## Portfolio Examination Part 1\n",
    "#### Janosch HÃ¶fer, 938969\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Imports](#imports) <br>\n",
    "- [1. Exercise](#task1) <br>\n",
    "- [2. Exercise](#task2) <br>\n",
    "- [3. Exercise](#task3) <br>\n",
    "- [4. Exercise](#task4) <br>\n",
    "-[References](#ref)<br>\n",
    "\n",
    "<a id='imports'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c353bb2-3b33-473c-ae7e-b5b7832af817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a77d79-50ec-4336-92f8-d85948b9e620",
   "metadata": {},
   "source": [
    "<a id='task1'></a>\n",
    "## Exercise 1\n",
    "Given a perceptron with weights $(0.1, 0.4, 0.6, 0.7)$ and bias $0.2$, compute the output for the tensor $\\begin{pmatrix}1 & 0 & 1 & 0\\\\0.1 & 0.2 & 0.1 & 0.2\\end{pmatrix}$ of dimensions (dataset, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23251b-c99c-40b3-8921-62ccf5c0f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.1, 0.4, 0.6, 0.7])\n",
    "bias = torch.tensor(0.2)\n",
    "vector = torch.tensor([[1, 0, 1, 0], [0.1, 0.2, 0.1, 0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf75f04-02ba-4185-b33e-aaa5e0965155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_predict(\n",
    "    input_t: torch.Tensor, weights: torch.Tensor, bias: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    return torch.matmul(input_t, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f655b-895d-4b51-b945-50cc290a710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = perceptron_predict(vector, weights, bias)\n",
    "output_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a42b08-e3d2-4d58-85e9-3c855b14cdd6",
   "metadata": {},
   "source": [
    "The result of the perceptron is a tensor with dimensionality (dataset). The results are 0.9 and 0.49."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74236ec5-efb9-401a-9664-21a89040f478",
   "metadata": {},
   "source": [
    "<a id='task2'></a>\n",
    "## Exercise 2\n",
    "The vacation platform JourneyAdvisor wants to apply deep learning in their recommender engine,\n",
    "that recommends points of interest to users based on their user account properties and previously\n",
    "visited places. The catalog of the platform contains $14,467$ points of interest. Users can check-in at\n",
    "such places using their phones. The platform has $1,989,345$ users. When users register, they enter\n",
    "their birthday, a payment method and their home address.<br>\n",
    "1. Propose a list of features, suitable for the recommendation task. Explain your choice!\n",
    "\n",
    "Suitable features could be:\n",
    "* Age\n",
    "* Number of visits\n",
    "* Payment method (as embeddings)\n",
    "* Home address (postal code)\n",
    "\n",
    "Instead of using the birthday, we can calculate the age of the user. This has the advantage, that age is not only a continuous value. It also is a smaller number than the birthday. The number of visits is also a continuous value that should be used for recommendations. If a user visits types of places more often, we can recommend places that users with similar preferences have visited. The payment method could be an interesting feature. Someone who uses Apple Pay would probably prefer to visit places, that offer this payment method. Whether a place offers Apple Pay can be inferred from other users who have visited that place. The difficulty here is that the payment method is a categorical value. To make this feature usable by our neural network, it has to be transformed, either using One-Hot Encoding or Embeddings. The last feature is the home address, more precisely the zip code. Here we have the same problem, that the zip code is a categorical value. But fortunately we have multiple solutions for that problem. The first approach uses continuous data that can be assigned to the various zip codes, e.g. average salary, crime-rate, house prices, etc. The second approach uses the latitude and longitude. In both approaches we can further fine tune the data, by changing the granularity of the zip code to look at districts, cities or states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e0652-d302-4621-8113-098c66908717",
   "metadata": {},
   "source": [
    "2. Describe a tensor to model the data for JourneyAdvisor. Describe its dimensionality.\n",
    "\n",
    "$T\\begin{matrix}( POI,&&    User,    &&& features)\\end{matrix}$<br>\n",
    "$T\\begin{matrix}( 14,467,& 1,989,345, & 4&)\\end{matrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa97dd-a6b8-4b7f-a9d8-cac4412ca767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "155c71ec-a4ff-4b86-ad6b-81d75ae59e0b",
   "metadata": {},
   "source": [
    "3. How many entries does the tensor have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9946a4-46ee-4409-833d-43eecccf19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{14467 * 1989345 * 4:,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85d2fe-8cb3-4760-8a97-fae023b34cc2",
   "metadata": {},
   "source": [
    "With the above dimensionality the total entries are 115,119,416,460. That's 115 Billion entries. Each additional feature increases it by ca. 28 Billion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da4288-c95b-4ff2-9006-1bdeeb346aa8",
   "metadata": {},
   "source": [
    "<a id='task3'></a>\n",
    "## Exercise 3\n",
    "Familiarize yourself with the SMOTE [[1]](#1) algorithm. In your own words, describe the use-case of the\n",
    "SMOTE. Among others, address these points:\n",
    "1. In which situations can it be useful (explain in general and provide three examples)?\n",
    "\n",
    "Synthetic Minority Oversampling Technique (SMOTE) uses statistical techniques to increase the number of\n",
    "underrepresented labels.\n",
    "\n",
    "2. What is its fundamental idea?\n",
    "\n",
    "SMOTE does not simply duplicated the underrepresented samples. It takes these samples' features and\n",
    "combines them with the features of the neighbor samples to generate new instances. \n",
    "\n",
    "3. How is SMOTE different from oversampling with replacement?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dabf608-3342-4ceb-ae48-b1c08918d1a6",
   "metadata": {},
   "source": [
    "<a id='task4'></a>\n",
    "## Exercise 4\n",
    "Create a Jupyter Notebook to solve the following machine learning task in Python, using PyTorch\n",
    "(and other suitable libraries):\n",
    "### 1. Load and arrange the dataset *portfolio_data_sose_2022.csv*. It has two features, feature_1 and feature_2, and a target variable target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b21347-ace8-4547-803c-539833df6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/portfolio_data_sose_2022.csv\")\n",
    "print(\n",
    "    f\"Number of rows: \\t{df.shape[0]} \\n\"\n",
    "    f\"Number of columns: \\t{df.shape[1]}\\n\"\n",
    "    f\"Number of targets: \\t{df['target'].nunique()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb6d8e-cf38-496e-bce4-91b4eb4193f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84d65a-74d3-429c-8b9e-9af4193f71bc",
   "metadata": {},
   "source": [
    "### 2. Describe the class distribution.\n",
    "\n",
    "The classes are highly imbalanced. Out of the 10.000 entries target 0 accounts for 9.900. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99688a1-62ba-498f-ab14-6b325bee78cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe12a2-daa9-41cc-8513-9bd2541976b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bar = sns.countplot(data=df, x=\"target\")\n",
    "bar.set(xlabel=\"Target\", ylabel=\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe7ccc-c6d8-4b1b-ace9-ef885b3e371f",
   "metadata": {},
   "source": [
    "### 3. Plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a43f04-c9a8-4bbf-a2f4-852637354564",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dis = sns.jointplot(data=df, x=\"feature_1\", y=\"feature_2\", hue=\"target\", kind=\"scatter\")\n",
    "dis.set_axis_labels(\"Feature 1\", \"Feature 2\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bd49b-2806-4b26-b597-cb3d30e93df6",
   "metadata": {},
   "source": [
    "Plotting the data not only shows the imbalanced distribution of the two targets. With regard to feature 1 and 2, the data points of both targets can hardly be separated from each other. This could pose a problem for the classification point at hand, because no more features are available for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d8a75-3e56-480f-bddb-a47ca88e5866",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dis2 = sns.relplot(x=\"feature_1\", y=\"feature_2\", data=df, col=\"target\", hue=\"target\")\n",
    "dis2.set_axis_labels(\"Feature 1\", \"Feature 2\", fontsize=12)\n",
    "dis2.set_titles(col_template=\"Target {col_name}\")\n",
    "dis2._legend.set_title(\"Target\")\n",
    "dis2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a25dc-5470-4527-b61a-7f79331a1269",
   "metadata": {},
   "source": [
    "### 4. Create a simple (single layer) neural network with two output neurons, one for each of the two classes 0 and 1 (i.e. use a multiclass classification setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fe5df-a2c3-4a74-abb1-5ba71d3d81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, classes: int):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(input_size, classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.lin1(x)\n",
    "\n",
    "    def predict(self, x: torch.Tensor):\n",
    "        _, indices = torch.max(self.forward(x), dim=1)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbd968-f4ee-490b-89f7-159f53a20cd5",
   "metadata": {},
   "source": [
    "The cross validation setting below trains 5 different models using the seeds 0 through 4. The averaged metrics for all 5 models are returned for further evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(\n",
    "    model_class: nn.Module,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "    lossfunc: nn.Module,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    ") -> dict[dict]:\n",
    "\n",
    "    X_train_t = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    X_test_t = torch.tensor(X_test.values, dtype=torch.float)\n",
    "    y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "    results = dict()\n",
    "    for seed in tqdm(range(5)):\n",
    "        torch.manual_seed(seed)\n",
    "        model, loss = train_model(model_class, X_train_t, y_train_t, lossfunc, lr, epochs)\n",
    "        results[f\"seed{seed}\"] = compute_acc(model, X_test_t, y_test_t)\n",
    "\n",
    "    results_avg = dict()\n",
    "    for metric in [\"Acc\", \"Bal_Acc\", \"Recall\", \"Precision\"]:\n",
    "        listy = [results[key][metric] for key in results.keys()]\n",
    "        results_avg[f\"Avg_{metric}\"] = sum(listy) / len(listy)\n",
    "    conf = [results[key][\"conf_matrix\"] for key in results.keys()]\n",
    "    results_avg[\"avg_conf\"] = np.mean(conf, axis=0)\n",
    "    return results_avg\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_class: nn.Module,\n",
    "    x_data: torch.Tensor,\n",
    "    y_data: torch.Tensor,\n",
    "    lossfunc: nn.Module,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    ") -> tuple[nn.Module, list]:\n",
    "\n",
    "    model = model_class(2, 2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss = [train_epoch(model, lossfunc, optimizer, x_data, y_data) for _ in range(epochs)]\n",
    "    return model, loss\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module, lossfunc: nn.Module, optimizer, x_data, y_data):\n",
    "    model.train()\n",
    "    y_train_pred = model(x_data)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = lossfunc(y_train_pred, y_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d4665-af87-4324-b748-fc44cc3b9718",
   "metadata": {},
   "source": [
    "To evaluate the model the following metrics are used:\n",
    "- Accuracy:<br>\n",
    "    $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "- Balanced Accuracy:<br>\n",
    "    $\\frac{1}{2} * (sensitivity + specificity)$ <br>\n",
    "    with $sensitivity = \\frac{TP}{TP + FN}$ <br>\n",
    "    with $specificity = \\frac{TN}{TN + FP}$\n",
    "    \n",
    "- Recall:<br>\n",
    "    $\\frac{TP}{TP + FN}$\n",
    "\n",
    "- Precision:<br>\n",
    "    $\\frac{TP}{TP + FP}$\n",
    "\n",
    "- Confusion Matrix:<br>\n",
    "    $\\begin{pmatrix}TN & FP \\\\FN & TP \\end{pmatrix}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31565d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(model: nn.Module, X: torch.Tensor, y: torch.Tensor) -> dict[str, float]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model.predict(X)\n",
    "    results = dict()\n",
    "    results[\"Acc\"] = metrics.accuracy_score(y, y_pred)\n",
    "    results[\"Bal_Acc\"] = metrics.balanced_accuracy_score(y, y_pred)\n",
    "    results[\"Recall\"] = metrics.recall_score(y, y_pred)\n",
    "    results[\"Precision\"] = metrics.precision_score(y, y_pred)\n",
    "    results[\"conf_matrix\"] = metrics.confusion_matrix(\n",
    "        y, y_pred, labels=y.unique(), normalize=\"true\"\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c060d",
   "metadata": {},
   "source": [
    "### 5. Compare the performance of the neural network in three different settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe669a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "learning_rate = 0.1\n",
    "folds = 5\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "data = df.iloc[:, :2]\n",
    "labels = df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7d494-8858-49b5-8b3e-e62e4ba00c78",
   "metadata": {},
   "source": [
    "For the first setting the plain data is used. The data is split five times into training and test datasets and each training dataset is trained using the cross validation setup.<br>\n",
    "\n",
    "DISCLAIMER: After a lot of discussion with my fellow students and Your feedback, I have come to the conclusion to use the setup below. Instead of an initial split into training and test data, the complete dataset is used for the five folds. This seems to be the most sensible approach with the instructions at hand, because we are not optimizing hyper-parameters. Splitting into training data before the splits would result in two different sets of test data, the initial test dataset (1) and a test dataset (2) for each fold. Here, the cross validation would be used to optimize hyper-parameters using test data (2), which are then used to train a model which can be evaluated with the initial test dataset (1). This approach would result in one model for each setting instead of 25.\n",
    "\n",
    "#### Plain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b240acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_results = dict()\n",
    "\n",
    "splits = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "pbar = tqdm(total=folds)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(splits.split(data, labels)):\n",
    "    X_train, X_test = data.iloc[train_idx], data.iloc[test_idx]\n",
    "    y_train, y_test = labels.iloc[train_idx], labels.iloc[test_idx]\n",
    "\n",
    "    tqdm.write(f\"Starting crossvalidation with fold {fold+1}\")\n",
    "    normal_results[f\"fold{fold}\"] = cross_validation(\n",
    "        model_class=NNClassifier,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        lossfunc=loss_function,\n",
    "        lr=learning_rate,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    pbar.update()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8f399-30a8-48d7-8ab3-389a3c9954fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = pd.DataFrame(normal_results).T.drop(columns=\"avg_conf\")\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac11f58-4c46-4b1a-8a0b-7ea7d5a01123",
   "metadata": {},
   "source": [
    "Using the plain data the average Accuracy for each folds spans between $0.9935 - 0.996$, the average Balanced Accuracy between $0.70 - 0.82$, the average Recall between $0.40 - 0.65$ and average Precision between $0.77 - 1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0913e7-121f-412f-aabd-3bd454bb3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_avg = pd.DataFrame(metric_df.mean(), columns=[\"Normal\"]).T\n",
    "metric_df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da88972-b41b-4c58-945a-e5a9b5db4fe6",
   "metadata": {},
   "source": [
    "The average metrics across all folds are Accuracy: $0.995$, Balanced Accuracy: $0.76$, Recall: $0.52$ and Precision: $0.89$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32cc435-cbb3-4e42-a0e5-98a002fa144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = [normal_results[key][\"avg_conf\"] for key in normal_results.keys()]\n",
    "metrics.ConfusionMatrixDisplay(np.mean(conf, axis=0)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f770267e-01c9-4266-a6e1-ef8e053dd3d3",
   "metadata": {},
   "source": [
    "The above confusion matrix displays the percentage of correctly/wrongly predicted targets. Each row sums up to 100%. Target 0 has been wrongly predicted 0,071% and target 1 has been wrongly predicted 48% times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3d737-0a42-4338-a851-5be55a96819f",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e30e54-6d92-4cb2-85d1-4e96278f3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sm = SMOTE(sampling_strategy=\"not majority\", random_state=42, n_jobs=-2)\n",
    "data_res, labels_res = over_sm.fit_resample(data, labels)\n",
    "df_res = data_res.assign(target=labels_res.values)\n",
    "\n",
    "dis3 = sns.jointplot(data=df_res, x=\"feature_1\", y=\"feature_2\", hue=\"target\", kind=\"scatter\")\n",
    "dis3.set_axis_labels(\"Feature 1\", \"Feature 2\", fontsize=12)\n",
    "plt.show(dis3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd3f4a-9c2a-4447-8f8c-923ec9b9ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_results = dict()\n",
    "\n",
    "splits = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "pbar = tqdm(total=folds)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(splits.split(data, labels)):\n",
    "    X_train, X_test = data.iloc[train_idx], data.iloc[test_idx]\n",
    "    y_train, y_test = labels.iloc[train_idx], labels.iloc[test_idx]\n",
    "\n",
    "    X_train_res, y_train_res = over_sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    tqdm.write(f\"Starting crossvalidation with fold {fold+1}\")\n",
    "    smote_results[f\"fold{fold}\"] = cross_validation(\n",
    "        model_class=NNClassifier,\n",
    "        X_train=X_train_res,\n",
    "        y_train=y_train_res,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        lossfunc=loss_function,\n",
    "        lr=learning_rate,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    pbar.update()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d862d92-9fd7-45a0-a258-ebd2704ac222",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_smote = pd.DataFrame(smote_results).T.drop(columns=\"avg_conf\")\n",
    "metric_df_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63dd885-7e92-406f-87e7-da4c23311353",
   "metadata": {},
   "source": [
    "Using the plain data the average Accuracy for each folds spans between $0.909 - 0.938$, the average Balanced Accuracy between $0.84 - 0.93$, the average Recall between $0.75 - 0.95$ and average Precision between $0.091 - 0.118$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779481c7-b84c-49da-8f3c-3a009a250221",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_smote_avg = pd.DataFrame(metric_df_smote.mean(), columns=[\"Smote\"]).T\n",
    "metric_df_smote_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d84a7-3142-4be9-ace4-a6ab7ee1e05f",
   "metadata": {},
   "source": [
    "The average metrics across all folds are Accuracy: $0.925$, Balanced Accuracy: $0.89$, Recall: $0.86$ and Precision: $0.11$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fad813-ba9a-42fd-8255-6468538b43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = [smote_results[key][\"avg_conf\"] for key in smote_results.keys()]\n",
    "metrics.ConfusionMatrixDisplay(np.mean(conf, axis=0)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cae37-9037-4573-8d24-26330a0574f9",
   "metadata": {},
   "source": [
    "Target 0 has been wrongly predicted 7,5% and target 1 has been wrongly predicted 14% times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c7dd8-5fb0-4ec2-ab32-14e138748a69",
   "metadata": {},
   "source": [
    "#### Using appropriate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc44e5-3b48-4459-9f89-2b0b7c5783be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=labels.unique(), y=labels)\n",
    "class_weights_t = torch.Tensor(class_weights)\n",
    "\n",
    "loss_function_w = nn.CrossEntropyLoss(class_weights_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db542ab9-370c-493e-87cf-92a75d064b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_results = dict()\n",
    "\n",
    "splits = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "pbar = tqdm(total=folds)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(splits.split(data, labels)):\n",
    "    X_train, X_test = data.iloc[train_idx], data.iloc[test_idx]\n",
    "    y_train, y_test = labels.iloc[train_idx], labels.iloc[test_idx]\n",
    "\n",
    "    tqdm.write(f\"Starting crossvalidation with fold {fold+1}\")\n",
    "    weight_results[f\"fold{fold}\"] = cross_validation(\n",
    "        model_class=NNClassifier,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        lossfunc=loss_function_w,\n",
    "        lr=learning_rate,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    pbar.update()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e770c2-08f8-4673-beae-f425c143b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_weight = pd.DataFrame(weight_results).T.drop(columns=\"avg_conf\")\n",
    "metric_df_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b70b0-e98b-4f49-93b2-3a9ff5be5582",
   "metadata": {},
   "source": [
    "Using the plain data the average Accuracy for each folds spans between $0.895 - 0.927$, the average Balanced Accuracy between $0.84 - 0.93$, the average Recall between $0.75 - 0.95$ and average Precision between $0.079 - 0.098$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64feebe5-adf2-4b53-85af-b0d804632262",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_weight_avg = pd.DataFrame(metric_df_weight.mean(), columns=[\"Weights\"]).T\n",
    "metric_df_weight_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c40c8b-b4dd-4daf-b524-3b33d7113e3d",
   "metadata": {},
   "source": [
    "The average metrics across all folds are Accuracy: $0.914$, Balanced Accuracy: $0.88$, Recall: $0.85$ and Precision: $0.09$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6d64d-3b86-4a7e-a595-eddafb9f6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = [weight_results[key][\"avg_conf\"] for key in weight_results.keys()]\n",
    "metrics.ConfusionMatrixDisplay(np.mean(conf, axis=0)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce327d6-1ad5-4c6c-b278-953cd9d6b668",
   "metadata": {},
   "source": [
    "Target 0 has been wrongly predicted 8,5% and target 1 has been wrongly predicted 15% times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923964dc-24ad-4f65-b53a-aca9c9535883",
   "metadata": {},
   "source": [
    "### 7. Interpret your results, explain your conclusions regarding SMOTE and class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4f9ec-61d6-4900-b9fa-ca5654a55a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([metric_df_avg, metric_df_smote_avg, metric_df_weight_avg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e74f48-1d0a-4b02-9d2f-027d518fb3c5",
   "metadata": {},
   "source": [
    " $\\begin{pmatrix}TN & FP \\\\FN & TP \\end{pmatrix}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004d74e-22e3-4350-b144-32c310404a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Baseline\n",
    "# (tp + tn) / (tp + tn + fp + fn)\n",
    "(0 + 9900) / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a44c1-fac6-4498-8b20-78d9d786c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Accuracy Baseline\n",
    "#        sensitivity   specificity\n",
    "# 1/2 * (tp/(tp+fn) + tn/(tn+fp))\n",
    "0.5 * (0 / (0 + 100) + 9900 / (9900 + 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2e487-b798-43d2-bb7c-1649bbacae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall Baseline\n",
    "# tp / (tp + fn)\n",
    "# Recall of the minority class\n",
    "0 / (0 + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4e8fa-db0f-40a4-968f-6659ffca8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Baseline\n",
    "# tp / (tp + fp)\n",
    "# Predicted as minority and correct\n",
    "100 / (100 + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac5d6d-70d5-4be0-acae-8d22a610e40e",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "## References\n",
    "<a id='1'>N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, âSmote: Synthetic minority\n",
    "over-sampling technique,â Journal of Artificial Intelligence Research, vol. 16, pp. 321â357, 2002.</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
