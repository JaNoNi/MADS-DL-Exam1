{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca6b724-2fd5-46cf-90fd-d4a5b68b50ed",
   "metadata": {},
   "source": [
    "# MADS-Deep Learning\n",
    "---\n",
    "## Portfolio Examination Part 1\n",
    "#### Janosch HÃ¶fer, 938969\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Imports](#imports) <br>\n",
    "- [1. Exercise](#task1) <br>\n",
    "- [2. Exercise](#task2) <br>\n",
    "- [3. Exercise](#task3) <br>\n",
    "- [4. Exercise](#task4) <br>\n",
    "-[References](#ref)<br>\n",
    "\n",
    "<a id='imports'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c353bb2-3b33-473c-ae7e-b5b7832af817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a77d79-50ec-4336-92f8-d85948b9e620",
   "metadata": {},
   "source": [
    "<a id='task1'></a>\n",
    "## Exercise 1\n",
    "Given a perceptron with weights $(0.1, 0.4, 0.6, 0.7)$ and bias $0.2$, compute the output for the tensor $\\begin{pmatrix}1 & 0 & 1 & 0\\\\0.1 & 0.2 & 0.1 & 0.2\\end{pmatrix}$ of dimensions (dataset, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23251b-c99c-40b3-8921-62ccf5c0f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.1, 0.4, 0.6, 0.7])\n",
    "bias = torch.tensor(0.2)\n",
    "vector = torch.tensor([[1, 0, 1, 0], [0.1, 0.2, 0.1, 0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf75f04-02ba-4185-b33e-aaa5e0965155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_predict(\n",
    "    input_t: torch.Tensor, weights: torch.Tensor, bias: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    return torch.matmul(input_t, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f655b-895d-4b51-b945-50cc290a710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = perceptron_predict(vector, weights, bias)\n",
    "output_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a42b08-e3d2-4d58-85e9-3c855b14cdd6",
   "metadata": {},
   "source": [
    "The result of the perceptron is a tensor with dimensionality (dataset). The results are 0.9 and 0.49."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74236ec5-efb9-401a-9664-21a89040f478",
   "metadata": {},
   "source": [
    "<a id='task2'></a>\n",
    "## Exercise 2\n",
    "The vacation platform JourneyAdvisor wants to apply deep learning in their recommender engine,\n",
    "that recommends points of interest to users based on their user account properties and previously\n",
    "visited places. The catalog of the platform contains $14,467$ points of interest. Users can check-in at\n",
    "such places using their phones. The platform has $1,989,345$ users. When users register, they enter\n",
    "their birthday, a payment method and their home address.<br>\n",
    "1. Propose a list of features, suitable for the recommendation task. Explain your choice!\n",
    "\n",
    "Suitable features could be:\n",
    "* Age\n",
    "* Number of visits\n",
    "* Payment method (as embeddings)\n",
    "* Home address (postal code)\n",
    "\n",
    "Instead of using the birthday, we can calculate the age of the user. This has the advantage, that age is not only a continuous value. It also is a smaller number than the birthday. The number of visits is also a continuous value that should be used for recommendations. If a user visits types of places more often, we can recommend places that users with similar preferences have visited. The payment method could be an interesting feature. Someone who uses Apple Pay would probably prefer to visit places, that offer this payment method. Whether a place offers Apple Pay can be inferred from other users who have visited that place. The difficulty here is that the payment method is a categorical value. To make this feature usable by our neural network, it has to be transformed, either using One-Hot Encoding or Embeddings. The last feature is the home address, more precisely the zip code. Here we have the same problem, that the zip code is a categorical value. But fortunately we have multiple solutions for that problem. The first approach uses continuous data that can be assigned to the various zip codes, e.g. average salary, crime-rate, house prices, etc. The second approach uses the latitude and longitude. In both approaches we can further fine tune the data, by changing the granularity of the zip code to look at districts, cities or states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2fe3b-fdb1-4a3e-9f0a-162a060b25f3",
   "metadata": {},
   "source": [
    "2. Describe a tensor to model the data for JourneyAdvisor. Describe its dimensionality.\n",
    "\n",
    "$T\\begin{matrix}( POI,&&    User,    &&& features)\\end{matrix}$<br>\n",
    "$T\\begin{matrix}( 14,467,& 1,989,345, & 4&)\\end{matrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa97dd-a6b8-4b7f-a9d8-cac4412ca767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "155c71ec-a4ff-4b86-ad6b-81d75ae59e0b",
   "metadata": {},
   "source": [
    "3. How many entries does the tensor have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9946a4-46ee-4409-833d-43eecccf19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{14467 * 1989345 * 4:,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85d2fe-8cb3-4760-8a97-fae023b34cc2",
   "metadata": {},
   "source": [
    "With the above dimensionality the total entries are 115,119,416,460. That's 115 Billion entries. Each additional feature increases it by ca. 28 Billion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da4288-c95b-4ff2-9006-1bdeeb346aa8",
   "metadata": {},
   "source": [
    "<a id='task3'></a>\n",
    "## Exercise 3\n",
    "Familiarize yourself with the SMOTE [[1]](#1) algorithm. In your own words, describe the use-case of the\n",
    "SMOTE. Among others, address these points:\n",
    "1. In which situations can it be useful (explain in general and provide three examples)?\n",
    "2. What is its fundamental idea?\n",
    "3. How is SMOTE different from oversampling with replacement?\n",
    "Exercise 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dabf608-3342-4ceb-ae48-b1c08918d1a6",
   "metadata": {},
   "source": [
    "<a id='task4'></a>\n",
    "## Exercise 4\n",
    "Create a Jupyter Notebook to solve the following machine learning task in Python, using PyTorch\n",
    "(and other suitable libraries):\n",
    "1. Load and arrange the dataset *portfolio_data_sose_2022.csv*. It has two features, feature_1 and feature_2, and a target variable target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b21347-ace8-4547-803c-539833df6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"data/portfolio_data_sose_2022.csv\")\n",
    "print(\n",
    "    f\"Number of rows: \\t{raw_df.shape[0]} \\n\"\n",
    "    f\"Number of columns: \\t{raw_df.shape[1]}\\n\"\n",
    "    f\"Number of targets: \\t{raw_df['target'].nunique()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb6d8e-cf38-496e-bce4-91b4eb4193f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce6360-bf92-4c6b-8ebb-2826b4ca01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_df.iloc[:, :2]\n",
    "labels = raw_df[\"target\"]\n",
    "data_train, data_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.33, random_state=42, shuffle=True, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b9075-f465-4c3b-8e8b-3e0454b057b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(), y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84d65a-74d3-429c-8b9e-9af4193f71bc",
   "metadata": {},
   "source": [
    "2. Describe the class distribution.\n",
    "\n",
    "The classes are highly imbalanced. Out of the 10.000 entries target 0 accounts for 9.900. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99688a1-62ba-498f-ab14-6b325bee78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe12a2-daa9-41cc-8513-9bd2541976b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=raw_df, x=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe7ccc-c6d8-4b1b-ace9-ef885b3e371f",
   "metadata": {},
   "source": [
    "3. Plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a43f04-c9a8-4bbf-a2f4-852637354564",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=raw_df, x=\"feature_1\", y=\"feature_2\", hue=\"target\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d8a75-3e56-480f-bddb-a47ca88e5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"feature_1\", y=\"feature_2\", data=raw_df, col=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a25dc-5470-4527-b61a-7f79331a1269",
   "metadata": {},
   "source": [
    "4. Create a simple (single layer) neural network with two output neurons, one for each of the two classes 0 and 1 (i.e. use a multiclass classification setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fe5df-a2c3-4a74-abb1-5ba71d3d81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def predict(self, x: torch.Tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac5d6d-70d5-4be0-acae-8d22a610e40e",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "## References\n",
    "<a id='1'>N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, âSmote: Synthetic minority\n",
    "over-sampling technique,â Journal of Artificial Intelligence Research, vol. 16, pp. 321â357, 2002.</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
